{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the digits database and scikit-learn\n",
    "\n",
    "1. Split your data into 30% test and 70% training sets, \n",
    "2. For each of the values of $C = 10^{k}$ for $k = {-10,...,10}$ train an $L^{2}$ regularized logistic regression model with regularization weight lambda = $\\frac{1}{C}$  (this is the default form for scikit-learn) on the training set and compute the mean accuracy on the test set for each model.  Which performed best?  \n",
    "3. Repeat #2 with $L^{1}$ regularization instead of $L^{2}$.  Do the results suggest any features that can be dropped from the data set?\n",
    "4. Scikit-learn does not have logistic regression without regularization.  What values of $C$ are most similar to an un-regularized model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "n,m = data.shape\n",
    "train_size = (7*n)//10\n",
    "train_x = data[:train_size,:]\n",
    "train_y = digits.target[:train_size]\n",
    "test = data[train_size:,:]\n",
    "test_act = digits.target[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best one was C=10, with an accuracy of 0.9148148148148149.\n"
     ]
    }
   ],
   "source": [
    "C = [10**k for k in range(-10,11,1)]\n",
    "accuracies = []\n",
    "for c in C :\n",
    "    classifier = LogisticRegression(C=1/c)\n",
    "    classifier.fit(train_x,train_y)\n",
    "    res = classifier.predict(test)\n",
    "    acc = 1-np.count_nonzero([res[i]-test_act[i] for i in range(len(res))])/len(res)\n",
    "    accuracies.append(acc)\n",
    "accur = max(accuracies)\n",
    "ind = accuracies.index(accur)\n",
    "print('The best one was C={}, with an accuracy of {}.'.format(C[ind],accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best one was C=10, with an accuracy of 0.9203703703703704.\n"
     ]
    }
   ],
   "source": [
    "C = [10**k for k in range(-10,11,1)]\n",
    "accuracies = []\n",
    "coeffs = {}\n",
    "for c in C :\n",
    "    classifier = LogisticRegression(penalty='l1',C=1/c)\n",
    "    classifier.fit(train_x,train_y)\n",
    "    res = classifier.predict(test)\n",
    "    acc = 1-np.count_nonzero([res[i]-test_act[i] for i in range(len(res))])/len(res)\n",
    "    accuracies.append(acc)\n",
    "    coeffs[c] = classifier.coef_\n",
    "accur = max(accuracies)\n",
    "ind = accuracies.index(accur)\n",
    "print('The best one was C={}, with an accuracy of {}.'.format(C[ind],accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.16608656e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.66854393e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.21280956e-02   0.00000000e+00\n",
      "    0.00000000e+00   2.01381443e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -1.25203503e-01\n",
      "   -4.86223370e-01   0.00000000e+00   5.96869991e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -6.21714254e-02\n",
      "   -3.39824533e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.55788903e-01  -1.56775978e-01\n",
      "   -1.05180496e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -3.18390137e-02   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.78304163e-02\n",
      "   -2.78998154e-01   7.15152591e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -2.29935593e-01  -2.29349599e-01  -2.52992039e-01\n",
      "   -3.78276632e-03   3.90108139e-02  -1.51271590e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.53711950e-01\n",
      "    1.57550547e-01  -2.40963923e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   4.15426044e-02   1.77948345e-01\n",
      "    0.00000000e+00   0.00000000e+00  -2.28497922e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -5.78018621e-03  -4.65359657e-02\n",
      "    5.40006751e-02  -3.02101202e-02  -1.82364615e-01   0.00000000e+00\n",
      "    0.00000000e+00  -2.32475400e-01  -6.79153402e-02   0.00000000e+00\n",
      "    0.00000000e+00  -1.65564395e-01  -1.36004772e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -3.85660093e-02   1.11625739e-01\n",
      "    5.41159444e-02   0.00000000e+00  -7.76358608e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -1.16693576e-01   0.00000000e+00\n",
      "    0.00000000e+00   1.11890562e-02  -2.07776041e-02   6.27375933e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -3.50893592e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   4.22365877e-02   0.00000000e+00\n",
      "    5.27602870e-02  -6.93277632e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -3.54937625e-02\n",
      "    4.51482118e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.80319034e-01  -3.08935525e-01\n",
      "   -1.53430891e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -1.48248141e-01   0.00000000e+00\n",
      "   -3.07591039e-02  -1.64623126e-01  -2.57626146e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.01944499e-04   1.14736529e-01\n",
      "   -1.90554374e-01  -3.56742473e-01  -3.91653566e-02   0.00000000e+00\n",
      "    0.00000000e+00   6.83276024e-02   0.00000000e+00   1.20298549e-01\n",
      "    9.68541319e-02   1.66450193e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.53540570e-02   0.00000000e+00\n",
      "    0.00000000e+00   6.36472670e-02   1.44250640e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    1.69118795e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -2.50351003e-02\n",
      "    0.00000000e+00   0.00000000e+00   2.83130774e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -9.67945107e-02  -1.92254451e-01\n",
      "    1.10577545e-01  -2.14270124e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -3.58796778e-01  -2.61107946e-02\n",
      "    0.00000000e+00  -1.73685397e-01  -1.91068409e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.03819539e-01   0.00000000e+00\n",
      "    1.07937924e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.96347138e-02  -5.59994661e-01\n",
      "    0.00000000e+00   1.27046675e-01   1.81942276e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   5.31794514e-03   0.00000000e+00\n",
      "    0.00000000e+00  -1.13466522e-02  -3.07711629e-02  -1.42494559e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00  -2.24676645e-01\n",
      "    0.00000000e+00  -2.40791651e-01  -9.82312849e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -3.20608757e-02   0.00000000e+00\n",
      "   -9.82662956e-02  -1.18023446e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.37827525e-01  -1.72271557e-02\n",
      "    0.00000000e+00   3.13288073e-02   1.05868675e-01   0.00000000e+00\n",
      "    0.00000000e+00   1.62228232e-01   3.99405381e-02   0.00000000e+00\n",
      "    4.59116476e-03   5.54695098e-02   7.60028897e-03   0.00000000e+00\n",
      "    0.00000000e+00   2.25000454e-01   0.00000000e+00   1.22660904e-01\n",
      "    8.20085826e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.03078659e-01  -3.79007587e-02\n",
      "    0.00000000e+00  -5.59081308e-02  -1.09223765e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -1.57638042e-01  -3.09305042e-02\n",
      "   -2.83542009e-03  -2.31516577e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.90373293e-01  -5.26628435e-02\n",
      "    0.00000000e+00   1.03489939e-01   1.95416390e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.44150523e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.35470281e-01   1.59168031e-03\n",
      "   -1.73026919e-01  -3.22127126e-01  -5.47838762e-01   0.00000000e+00\n",
      "    0.00000000e+00   1.85896057e-02   1.12846511e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -8.79156514e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -4.43218723e-03\n",
      "   -1.44862489e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.40251110e-01  -1.46192384e-01\n",
      "   -1.17667612e-03   4.05532241e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.27686310e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.26508576e-02   0.00000000e+00\n",
      "   -8.55079975e-02  -5.81359872e-02  -1.44426550e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00  -1.62242003e-01   0.00000000e+00\n",
      "   -7.97469176e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -4.61437242e-02   0.00000000e+00   0.00000000e+00\n",
      "   -1.11826244e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.83355816e-01  -4.71751299e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -3.82109033e-02   0.00000000e+00  -2.32514134e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   4.38837696e-02   6.50740366e-04\n",
      "   -1.78282224e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -2.22515248e-01   3.25168766e-01   0.00000000e+00\n",
      "   -2.84210379e-02   0.00000000e+00   7.25682728e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.69417096e-02   4.52092430e-02\n",
      "   -1.31176883e-01   1.03613145e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -9.25180988e-02  -1.25836305e-01\n",
      "    0.00000000e+00   4.97355238e-02   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.57129833e-01   9.18475695e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    2.57980972e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.43258062e-02  -1.63364623e-01  -1.36473371e-01\n",
      "    0.00000000e+00   0.00000000e+00   4.16345466e-02   0.00000000e+00\n",
      "    0.00000000e+00  -1.97108139e-01  -1.33381879e-02  -2.01365342e-01\n",
      "    0.00000000e+00   4.84107265e-02   7.28558528e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   3.51183151e-04\n",
      "    1.09286342e-03   1.29692000e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    2.53000900e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -3.80151324e-02   2.60856655e-02\n",
      "   -2.27087039e-01  -3.73275216e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.05029251e-02   0.00000000e+00\n",
      "   -1.20666127e-01  -2.68152457e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00  -1.57814604e-01\n",
      "   -6.40759190e-02  -4.99530764e-03  -2.05012353e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.56319838e-01   0.00000000e+00\n",
      "   -1.18680609e-01   7.81286505e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   4.16256871e-02   6.04529780e-02   1.83841274e-02\n",
      "    0.00000000e+00   1.78922702e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.25575522e-02  -1.96429842e-02   1.25619260e-01\n",
      "   -6.92559952e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -2.88661882e-01  -9.02455887e-02   2.24231629e-01\n",
      "    0.00000000e+00  -1.82391838e-01  -4.91802729e-01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   9.12926132e-02   2.59269223e-04\n",
      "    1.31646517e-02   9.77000295e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -2.75460200e-02   1.16752624e-01  -1.83926256e-01\n",
      "   -1.17487985e-01   2.89006543e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.98728793e-01   0.00000000e+00\n",
      "   -2.99926621e-02  -1.05201222e-01  -1.12439979e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.64845926e-01  -8.12181684e-02  -4.06885031e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.93484819e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.93224742e-01  -5.83411137e-03\n",
      "    0.00000000e+00   3.74921633e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.20814760e-02   1.17473137e-01\n",
      "    0.00000000e+00   2.31951363e-01  -6.60458299e-02   0.00000000e+00\n",
      "    0.00000000e+00  -2.73978906e-01  -8.37271661e-02   1.65438252e-01\n",
      "   -1.41619931e-01  -7.89061367e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -4.00951570e-01  -3.49309141e-01\n",
      "   -2.70166573e-01  -1.36690334e-01  -6.42816268e-02   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -2.60459823e-02   0.00000000e+00\n",
      "   -7.12609645e-02  -6.57268372e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -5.37013472e-02  -8.41116547e-02\n",
      "    0.00000000e+00  -4.12638986e-02   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "best_c = 10\n",
    "print(coeffs[best_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it does.  It seems that it suggests that certain pixels (from observation mostly around the outside and a few near the middle) are unnecessary in determining the number.  This makes sense, as the those are probably spots that are always the background and never actually part of the digit.  \n",
    "I would say that the closest to one without a C would be the really large values of C, because then $\\lambda = \\frac{1}{C}$ is relatively small, meaning the penalized term hardly effects the minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a classification problem related to your final project, using your project data.  \n",
    "\n",
    "1. Apply $L^2$ regularized logistic regression to model this with an appropriate choice of $C$ (or lambda).  Discuss how (and why) you chose your specific the value $C$.  \n",
    "2. Apply $L^1$ regularized logistic regression to model this with an appropriate choice of $C$.  Discuss how (and why) you chose your specific value of $C$.  \n",
    "3. Identify which features of your data to include and which to discard for a good logistic regression model for your problem.  Compare which features are suggested for removal by $L^1$ regularization (from `scikit-learn) versus using the methods we have used for linear regression, including p-values, BIC, and AIC (from statsmodels).  \n",
    "Clearly identify your final preferred model, and explain why you chose that over the other contenders. \n",
    "What conclusions can be drawn from your results about the original classification question you asked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = '../../../../Senior Project/DATA/'\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Walk through player files\n",
    "for dir_path , dir_name , file_names in os.walk(path) :\n",
    "    # 2017 will be our testing set\n",
    "    if '2017' in dir_path :\n",
    "        for name in file_names :\n",
    "            # Grab avgs file\n",
    "            if name[-4:] == 'avgs' :\n",
    "                data = pd.read_csv(os.path.join(dir_path,name))\n",
    "                if isinstance(test,list) :\n",
    "                    test = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "                else :\n",
    "                    test = np.vstack((test,data.drop(['Unnamed: 0'],axis=1)))\n",
    "    # Everything else will become our training set\n",
    "    else :\n",
    "        for name in file_names :\n",
    "            # Grab avgs file\n",
    "            if name[-4:] == 'avgs' :\n",
    "                data = pd.read_csv(os.path.join(dir_path,name))\n",
    "                if isinstance(train,list) :\n",
    "                    train = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "                else :\n",
    "                    train = np.vstack((train,data.drop(['Unnamed: 0'],axis=1).as_matrix()))\n",
    "\n",
    "# From the way the data is saved, the last column is whether or not the player\n",
    "#     is a score on how much of a contributor he was during the season.\n",
    "\n",
    "# !!! NOTE !!! : This ranking is currently arbitrary, and as such has no current\n",
    "#                meaning.\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = [10**k for k in range(-10,11,1)]\n",
    "accuracies = []\n",
    "for c in C :\n",
    "    classifier = LogisticRegression(C=1/c)\n",
    "    classifier.fit(train_x,train_y)\n",
    "    res = classifier.predict(test)\n",
    "    acc = 1-np.count_nonzero([res[i]-test_act[i] for i in range(len(res))])/len(res)\n",
    "    accuracies.append(acc)\n",
    "accur = max(accuracies)\n",
    "ind = accuracies.index(accur)\n",
    "print('The best one was C={}, with an accuracy of {}.'.format(C[ind],accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = [10**k for k in range(-10,11,1)]\n",
    "accuracies = []\n",
    "coeffs = {}\n",
    "for c in C :\n",
    "    classifier = LogisticRegression(penalty='l1',C=1/c)\n",
    "    classifier.fit(train_x,train_y)\n",
    "    res = classifier.predict(test)\n",
    "    acc = 1-np.count_nonzero([res[i]-test_act[i] for i in range(len(res))])/len(res)\n",
    "    accuracies.append(acc)\n",
    "    coeffs[c] = classifier.coef_\n",
    "accur = max(accuracies)\n",
    "ind = accuracies.index(accur)\n",
    "print('The best one was C={}, with an accuracy of {}.'.format(C[ind],accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_c = ?\n",
    "print(coeffs[best_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above (for finding C) is to be run multiple times.  The first time as displayed, and then when promising intervals of C are revealed, run again with different C values from those intervals.  When the best C has been discovered (which would be as consistent as possible between $l^{2}$ and $l^{1}$), the coefficients will then determine which parameters are and are not applicable.  \n",
    "Since this ranking is my personal one and I haven't come up with a good way to score the players yet, the code is not run, as any computations on the data as is would be completely meaningless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
