{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import random_projection\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Apply PCA to the cancer dataset to reduce the dimension of the feature space to each of 15, 10, and 5.    Are there any features or combinations of features for which PCA is not a suitable method to use?  Explain.  WARNING: remember to center your data (subtract the mean) and also normalize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0             1             2             3             4   \\\n",
      "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
      "mean  -3.162867e-15 -6.530609e-15 -7.078891e-16 -8.799835e-16  6.132177e-15   \n",
      "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
      "min   -2.029648e+00 -2.229249e+00 -1.984504e+00 -1.454443e+00 -3.112085e+00   \n",
      "25%   -6.893853e-01 -7.259631e-01 -6.919555e-01 -6.671955e-01 -7.109628e-01   \n",
      "50%   -2.150816e-01 -1.046362e-01 -2.359800e-01 -2.951869e-01 -3.489108e-02   \n",
      "75%    4.693926e-01  5.841756e-01  4.996769e-01  3.635073e-01  6.361990e-01   \n",
      "max    3.971288e+00  4.651889e+00  3.976130e+00  5.250529e+00  4.770911e+00   \n",
      "\n",
      "                 5             6             7             8             9   \\\n",
      "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
      "mean  -1.120369e-15 -4.421380e-16  9.732500e-16 -1.971670e-15 -1.453631e-15   \n",
      "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
      "min   -1.610136e+00 -1.114873e+00 -1.261820e+00 -2.744117e+00 -1.819865e+00   \n",
      "25%   -7.470860e-01 -7.437479e-01 -7.379438e-01 -7.032397e-01 -7.226392e-01   \n",
      "50%   -2.219405e-01 -3.422399e-01 -3.977212e-01 -7.162650e-02 -1.782793e-01   \n",
      "75%    4.938569e-01  5.260619e-01  6.469351e-01  5.307792e-01  4.709834e-01   \n",
      "max    4.568425e+00  4.243589e+00  3.927930e+00  4.484751e+00  4.910919e+00   \n",
      "\n",
      "           ...                 20            21            22            23  \\\n",
      "count      ...       5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
      "mean       ...      -2.333224e-15  1.763674e-15 -1.198026e-15  5.049661e-16   \n",
      "std        ...       1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
      "min        ...      -1.726901e+00 -2.223994e+00 -1.693361e+00 -1.222423e+00   \n",
      "25%        ...      -6.749213e-01 -7.486293e-01 -6.895783e-01 -6.421359e-01   \n",
      "50%        ...      -2.690395e-01 -4.351564e-02 -2.859802e-01 -3.411812e-01   \n",
      "75%        ...       5.220158e-01  6.583411e-01  5.402790e-01  3.575891e-01   \n",
      "max        ...       4.094189e+00  3.885905e+00  4.287337e+00  5.930172e+00   \n",
      "\n",
      "                 24            25            26            27            28  \\\n",
      "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
      "mean  -5.213170e-15 -2.174788e-15  6.856456e-16 -1.412656e-16 -2.289567e-15   \n",
      "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
      "min   -2.682695e+00 -1.443878e+00 -1.305831e+00 -1.745063e+00 -2.160960e+00   \n",
      "25%   -6.912304e-01 -6.810833e-01 -7.565142e-01 -7.563999e-01 -6.418637e-01   \n",
      "50%   -4.684277e-02 -2.695009e-01 -2.182321e-01 -2.234689e-01 -1.274095e-01   \n",
      "75%    5.975448e-01  5.396688e-01  5.311411e-01  7.125100e-01  4.501382e-01   \n",
      "max    3.955374e+00  5.112877e+00  4.700669e+00  2.685877e+00  6.046041e+00   \n",
      "\n",
      "                 29  \n",
      "count  5.690000e+02  \n",
      "mean   2.575171e-15  \n",
      "std    1.000880e+00  \n",
      "min   -1.601839e+00  \n",
      "25%   -6.919118e-01  \n",
      "50%   -2.164441e-01  \n",
      "75%    4.507624e-01  \n",
      "max    6.846856e+00  \n",
      "\n",
      "[8 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "print(pd.DataFrame(data=x_scaled).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 15 : [ 0.97368421  0.97368421  0.96825397]\n",
      "n = 10 : [ 0.98947368  0.96842105  0.97354497]\n",
      "n = 5 : [ 0.96842105  0.97368421  0.96825397]\n"
     ]
    }
   ],
   "source": [
    "n_dimensions = [15,10,5]\n",
    "for n in n_dimensions :\n",
    "    pca = PCA(n_components=n)\n",
    "    x_new = pca.fit_transform(x_scaled)\n",
    "    lgr = LogisticRegression()\n",
    "    score = cross_val_score(lgr,x_new,y)\n",
    "    print('n = ' + str(n) + ' : ' + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Apply three of your favorite classification methods to the full cancer data set and also to the PCA-reduced data.  Analyze and evaluate the performance (time and accuracy) for each combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [ Naive Bayes       ,      Log Reg.        ,   SVM ]\n",
      "Time -\n",
      "     PCA : [0.00645899772644043, 0.01373910903930664, 0.056427001953125]\n",
      "     Reg : [0.0074460506439208984, 0.011440277099609375, 0.06028580665588379]\n",
      "Acc. -\n",
      "     PCA : [0.91916829109811571, 0.97714657012902617, 0.96308363501345962]\n",
      "     Reg : [0.92970388935301218, 0.975392184164114, 0.9736377981992016]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "pca = PCA(n_components=n)\n",
    "x_new = pca.fit_transform(x_scaled)\n",
    "pca_times = []\n",
    "pca_scores = []\n",
    "reg_times = []\n",
    "reg_scores = []\n",
    "\n",
    "gnb = GaussianNB()\n",
    "start = time()\n",
    "score = cross_val_score(gnb,x_new,y)\n",
    "end = time()\n",
    "pca_times.append(end-start)\n",
    "pca_scores.append(np.average(score))\n",
    "start = time()\n",
    "score = cross_val_score(gnb,x_scaled,y)\n",
    "end = time()\n",
    "reg_times.append(end-start)\n",
    "reg_scores.append(np.average(score))\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "start = time()\n",
    "score = cross_val_score(lgr,x_new,y)\n",
    "end = time()\n",
    "pca_times.append(end-start)\n",
    "pca_scores.append(np.average(score))\n",
    "start = time()\n",
    "score = cross_val_score(lgr,x_scaled,y)\n",
    "end = time()\n",
    "reg_times.append(end-start)\n",
    "reg_scores.append(np.average(score))\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "start = time()\n",
    "score = cross_val_score(svc,x_new,y)\n",
    "end = time()\n",
    "pca_times.append(end-start)\n",
    "pca_scores.append(np.average(score))\n",
    "start = time()\n",
    "score = cross_val_score(svc,x_scaled,y)\n",
    "end = time()\n",
    "reg_times.append(end-start)\n",
    "reg_scores.append(np.average(score))\n",
    "\n",
    "print(\"           [ Naive Bayes       ,      Log Reg.        ,   SVM ]\")\n",
    "print(\"Time -\")\n",
    "print(\"     PCA : {}\".format(pca_times))\n",
    "print(\"     Reg : {}\".format(reg_times))\n",
    "print(\"Acc. -\")\n",
    "print(\"     PCA : {}\".format(pca_scores))\n",
    "print(\"     Reg : {}\".format(reg_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Find some aspect of your final project for which PCA is an appropriate dimension-reduction method.  Apply PCA and analyze the results and performance.  Compare to your results without PCA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA -\n",
      "    Time  : 0.036509037017822266\n",
      "    Score : 0.8660235338931246\n",
      "Reg -\n",
      "    Time  : 0.035748958587646484\n",
      "    Score : 0.8681764588219645\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = '../../../../Senior Project/DATA/'\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Walk through player files\n",
    "for dir_path , dir_name , file_names in os.walk(path) :\n",
    "    for name in file_names :\n",
    "        # Grab avgs file\n",
    "        if name[-4:] == 'avgs' :\n",
    "            data = pd.read_csv(os.path.join(dir_path,name))\n",
    "            if isinstance(train,list) :\n",
    "                train = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "            else :\n",
    "                train = np.vstack((train,data.drop(['Unnamed: 0'],axis=1).as_matrix()))\n",
    "\n",
    "# From the way the data is saved, the last column is whether or not the player\n",
    "#     is a score on how much of a contributor he was during the season.\n",
    "x = train[:,:-1]\n",
    "y = train[:,-1]\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "\n",
    "n = 10\n",
    "pca = PCA(n_components=n)\n",
    "x_new = pca.fit_transform(x_scaled)\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "start = time()\n",
    "score = cross_val_score(lgr,x_new,y)\n",
    "end = time()\n",
    "pca_time = end-start\n",
    "pca_score = np.average(score)\n",
    "start = time()\n",
    "score = cross_val_score(lgr,x_scaled,y)\n",
    "end = time()\n",
    "reg_time = end-start\n",
    "reg_score = np.average(score)\n",
    "\n",
    "print('PCA -')\n",
    "print('    Time  : {}'.format(pca_time))\n",
    "print('    Score : {}'.format(pca_score))\n",
    "print('Reg -')\n",
    "print('    Time  : {}'.format(reg_time))\n",
    "print('    Score : {}'.format(reg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Repeat what you did in the previous problem, but replacing PCA by a random projection. Try 5 different random projections and compare the results and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "eps=0.100000 and n_samples=3717 lead to a target dimension of 7046 which is larger than the original space with n_features=19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-962f30490fd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mrand_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_projection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseRandomProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/random_projection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0;34m'%d which is larger than the original space with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                     'n_features=%d' % (self.eps, n_samples, self.n_components_,\n\u001b[0;32m--> 364\u001b[0;31m                                        n_features))\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: eps=0.100000 and n_samples=3717 lead to a target dimension of 7046 which is larger than the original space with n_features=19"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = '../../../../Senior Project/DATA/'\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Walk through player files\n",
    "for dir_path , dir_name , file_names in os.walk(path) :\n",
    "    for name in file_names :\n",
    "        # Grab avgs file\n",
    "        if name[-4:] == 'avgs' :\n",
    "            data = pd.read_csv(os.path.join(dir_path,name))\n",
    "            if isinstance(train,list) :\n",
    "                train = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "            else :\n",
    "                train = np.vstack((train,data.drop(['Unnamed: 0'],axis=1).as_matrix()))\n",
    "\n",
    "# From the way the data is saved, the last column is whether or not the player\n",
    "#     is a score on how much of a contributor he was during the season.\n",
    "x = train[:,:-1]\n",
    "y = train[:,-1]\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "\n",
    "rand_proj_times = []\n",
    "rand_proj_scores = []\n",
    "reg_times = []\n",
    "reg_scores = []\n",
    "\n",
    "for i in range(5) :\n",
    "    n = 10\n",
    "    rand_proj = random_projection.SparseRandomProjection()\n",
    "    x_new = rand_proj.fit_transform(x_scaled)\n",
    "\n",
    "    lgr = LogisticRegression()\n",
    "    start = time()\n",
    "    score = cross_val_score(lgr,x_new,y)\n",
    "    end = time()\n",
    "    rand_proj_times.append(end-start)\n",
    "    rand_proj_scores.append(np.average(score))\n",
    "    start = time()\n",
    "    score = cross_val_score(lgr,x_scaled,y)\n",
    "    end = time()\n",
    "    reg_times.append(end-start)\n",
    "    reg_scores.append(np.average(score))\n",
    "\n",
    "print('Rand. Proj -')\n",
    "print('    Time  : {}'.format(rand_proj_times))\n",
    "print('    Score : {}'.format(rand_proj_scores))\n",
    "print('Reg        -')\n",
    "print('    Time  : {}'.format(reg_times))\n",
    "print('    Score : {}'.format(reg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
