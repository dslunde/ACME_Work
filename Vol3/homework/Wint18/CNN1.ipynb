{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist as fmn\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution1D, MaxPooling1D, Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Build a CNN comparable in structure to the one in the reading you did on CNNs, but train and test it on the fashion-MNIST dataset.  Adjust some of the parameters and compare the results.  You should be able to get performance better than any of the classifiers we have used on Fashion-MNIST so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "(train_x,train_y),(test_x,test_y) = fmn.load_data()\n",
    "\n",
    "# Get important nums about data\n",
    "num_train, height, width = train_x.shape\n",
    "num_test = test_x.shape[0]\n",
    "num_classes = np.unique(train_y).shape[0]\n",
    "\n",
    "# Standardize data\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')\n",
    "train_x /= np.max(train_x)\n",
    "test_x /= np.max(test_x)\n",
    "\n",
    "train_y = to_categorical(train_y,num_classes)\n",
    "test_y = to_categorical(test_y,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up constants\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "conv_depth_1 = 32\n",
    "conv_depth_2 = 64\n",
    "drop_prob_1 = 0.25\n",
    "drop_prob_2 = 0.5\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your CNN.\n",
    "inputs = Input(shape=(height, width))\n",
    "conv_1 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(inputs)\n",
    "conv_2 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling1D(pool_size=pool_size)(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "conv_3 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(drop_1)\n",
    "conv_4 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling1D(pool_size=pool_size)(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes,activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.5761 - acc: 0.7893\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.4062 - acc: 0.8507\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.3719 - acc: 0.8625\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3514 - acc: 0.8689\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3368 - acc: 0.8753\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3280 - acc: 0.8787\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3189 - acc: 0.8821\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.3095 - acc: 0.8837\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.3067 - acc: 0.8867\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.3023 - acc: 0.8871\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2944 - acc: 0.8900\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.2938 - acc: 0.8899\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.2840 - acc: 0.8930\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.2857 - acc: 0.8944\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2832 - acc: 0.8941\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2804 - acc: 0.8960\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2791 - acc: 0.8960\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2789 - acc: 0.8962\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2771 - acc: 0.8954\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.2695 - acc: 0.8993\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2732 - acc: 0.8974\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.2703 - acc: 0.8982\n",
      "10000/10000 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28503741631507873, 0.896]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test.\n",
    "model.fit(train_x,train_y,batch_size=batch_size,epochs=num_epochs,verbose=1,callbacks=[early_stopping])\n",
    "model.evaluate(test_x,test_y,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.5668 - acc: 0.7893\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.4110 - acc: 0.8474\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.3761 - acc: 0.8593\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.3518 - acc: 0.8689\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.3383 - acc: 0.8746\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.3280 - acc: 0.8770\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.3212 - acc: 0.8798\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.3136 - acc: 0.8837\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.3075 - acc: 0.8849\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.3032 - acc: 0.8865\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.2973 - acc: 0.8883\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.2993 - acc: 0.8886\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.2938 - acc: 0.8902\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.2911 - acc: 0.8901\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.2927 - acc: 0.8896\n",
      "10000/10000 [==============================] - 1s 70us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3066774897933006, 0.8856]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust parameters.\n",
    "# Set up constants\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "kernel_size = 5\n",
    "pool_size = 3\n",
    "conv_depth_1 = 32\n",
    "conv_depth_2 = 64\n",
    "drop_prob_1 = 0.25\n",
    "drop_prob_2 = 0.35\n",
    "hidden_size = 1024\n",
    "# Build your CNN.\n",
    "inputs = Input(shape=(height, width))\n",
    "conv_1 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(inputs)\n",
    "conv_2 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling1D(pool_size=pool_size)(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "conv_3 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(drop_1)\n",
    "conv_4 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling1D(pool_size=pool_size)(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes,activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)\n",
    "# Train and test.\n",
    "model.fit(train_x,train_y,batch_size=batch_size,epochs=num_epochs,verbose=1,callbacks=[early_stopping])\n",
    "model.evaluate(test_x,test_y,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 51s 842us/step - loss: 0.5132 - acc: 0.8118\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 46s 767us/step - loss: 0.3749 - acc: 0.8610\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 46s 769us/step - loss: 0.3392 - acc: 0.8739\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 47s 783us/step - loss: 0.3159 - acc: 0.8819\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 47s 780us/step - loss: 0.2973 - acc: 0.8886\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 46s 759us/step - loss: 0.2826 - acc: 0.8939\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 50s 836us/step - loss: 0.2670 - acc: 0.8988\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 48s 803us/step - loss: 0.2574 - acc: 0.9029\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 44s 736us/step - loss: 0.2453 - acc: 0.9079\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 44s 738us/step - loss: 0.2354 - acc: 0.9121\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 44s 735us/step - loss: 0.2263 - acc: 0.9147\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 44s 736us/step - loss: 0.2174 - acc: 0.9173\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 44s 736us/step - loss: 0.2083 - acc: 0.9209\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 44s 737us/step - loss: 0.2027 - acc: 0.9249\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 44s 736us/step - loss: 0.1950 - acc: 0.9264\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 44s 736us/step - loss: 0.1874 - acc: 0.9299\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 44s 738us/step - loss: 0.1811 - acc: 0.9317\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 44s 739us/step - loss: 0.1756 - acc: 0.9337\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 44s 738us/step - loss: 0.1684 - acc: 0.9366\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 47s 777us/step - loss: 0.1699 - acc: 0.9369\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 46s 761us/step - loss: 0.1598 - acc: 0.9404\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 44s 739us/step - loss: 0.1565 - acc: 0.9408\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 44s 740us/step - loss: 0.1558 - acc: 0.9417\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 45s 754us/step - loss: 0.1499 - acc: 0.9438\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 45s 754us/step - loss: 0.1448 - acc: 0.9445\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 46s 769us/step - loss: 0.1412 - acc: 0.9462\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 45s 749us/step - loss: 0.1361 - acc: 0.9494\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 45s 753us/step - loss: 0.1372 - acc: 0.9476\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 45s 753us/step - loss: 0.1326 - acc: 0.9499\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 45s 754us/step - loss: 0.1284 - acc: 0.9525\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 46s 762us/step - loss: 0.1294 - acc: 0.9517\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 46s 763us/step - loss: 0.1219 - acc: 0.9549\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 49s 823us/step - loss: 0.1249 - acc: 0.9530\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 50s 827us/step - loss: 0.1229 - acc: 0.9553\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 50s 828us/step - loss: 0.1205 - acc: 0.9552\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 49s 810us/step - loss: 0.1165 - acc: 0.9560\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 47s 776us/step - loss: 0.1205 - acc: 0.9562\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 46s 772us/step - loss: 0.1166 - acc: 0.9568\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 46s 764us/step - loss: 0.1148 - acc: 0.9577\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 47s 778us/step - loss: 0.1117 - acc: 0.9587\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 47s 776us/step - loss: 0.1165 - acc: 0.9580\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 46s 762us/step - loss: 0.1093 - acc: 0.9604\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 47s 784us/step - loss: 0.1094 - acc: 0.9608\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 46s 767us/step - loss: 0.1070 - acc: 0.9609\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 47s 781us/step - loss: 0.1063 - acc: 0.9610\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 47s 791us/step - loss: 0.1038 - acc: 0.9624\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 49s 809us/step - loss: 0.1072 - acc: 0.9619\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 45s 752us/step - loss: 0.1041 - acc: 0.9627\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 47s 778us/step - loss: 0.1043 - acc: 0.9617\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 47s 790us/step - loss: 0.1007 - acc: 0.9630\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 48s 800us/step - loss: 0.1024 - acc: 0.9632\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 48s 800us/step - loss: 0.1016 - acc: 0.9645\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 47s 788us/step - loss: 0.0981 - acc: 0.9644\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 48s 792us/step - loss: 0.0981 - acc: 0.9649\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 46s 761us/step - loss: 0.1012 - acc: 0.9646\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 46s 760us/step - loss: 0.0975 - acc: 0.9655\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 46s 771us/step - loss: 0.0951 - acc: 0.9663\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 46s 774us/step - loss: 0.0950 - acc: 0.9659\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 49s 820us/step - loss: 0.0952 - acc: 0.9656\n",
      "10000/10000 [==============================] - 1s 126us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4067879297107458, 0.8965]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust parameters.\n",
    "# Set up constants\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "kernel_size = 2\n",
    "pool_size = 1\n",
    "conv_depth_1 = 16\n",
    "conv_depth_2 = 64\n",
    "drop_prob_1 = 0.15\n",
    "drop_prob_2 = 0.35\n",
    "hidden_size = 1024\n",
    "# Build your CNN.\n",
    "inputs = Input(shape=(height, width))\n",
    "conv_1 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(inputs)\n",
    "conv_2 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling1D(pool_size=pool_size)(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "conv_3 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(drop_1)\n",
    "conv_4 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling1D(pool_size=pool_size)(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes,activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)\n",
    "# Train and test.\n",
    "model.fit(train_x,train_y,batch_size=batch_size,epochs=num_epochs,verbose=1,callbacks=[early_stopping])\n",
    "model.evaluate(test_x,test_y,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.5725 - acc: 0.7906\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.3959 - acc: 0.8585\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 22s 365us/step - loss: 0.3563 - acc: 0.8721\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.3352 - acc: 0.8791\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.3195 - acc: 0.8842\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 23s 379us/step - loss: 0.3043 - acc: 0.8892\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.2974 - acc: 0.8919\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.2931 - acc: 0.8937\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.2831 - acc: 0.8974\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 22s 372us/step - loss: 0.2823 - acc: 0.8983\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.2753 - acc: 0.9003\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 22s 373us/step - loss: 0.2722 - acc: 0.9008\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 23s 375us/step - loss: 0.2648 - acc: 0.9041\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 22s 372us/step - loss: 0.2657 - acc: 0.9027\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 22s 375us/step - loss: 0.2619 - acc: 0.9060\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 23s 375us/step - loss: 0.2597 - acc: 0.9066\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 23s 379us/step - loss: 0.2576 - acc: 0.9069\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 23s 377us/step - loss: 0.2577 - acc: 0.9069\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.2565 - acc: 0.9078\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 22s 374us/step - loss: 0.2542 - acc: 0.9086\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 23s 377us/step - loss: 0.2532 - acc: 0.9105\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.2479 - acc: 0.9121\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 0.2474 - acc: 0.9105\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 24s 406us/step - loss: 0.2491 - acc: 0.9114\n",
      "10000/10000 [==============================] - 1s 123us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3061091561436653, 0.8971]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust parameters.\n",
    "# Set up constants\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "kernel_size = 8\n",
    "pool_size = 4\n",
    "conv_depth_1 = 64\n",
    "conv_depth_2 = 128\n",
    "drop_prob_1 = 0.25\n",
    "drop_prob_2 = 0.55\n",
    "hidden_size = 512\n",
    "# Build your CNN.\n",
    "inputs = Input(shape=(height, width))\n",
    "conv_1 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(inputs)\n",
    "conv_2 = Convolution1D(conv_depth_1,kernel_size,padding='same',activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling1D(pool_size=pool_size)(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "conv_3 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(drop_1)\n",
    "conv_4 = Convolution1D(conv_depth_2,kernel_size,padding='same',activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling1D(pool_size=pool_size)(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes,activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)\n",
    "# Train and test.\n",
    "model.fit(train_x,train_y,batch_size=batch_size,epochs=num_epochs,verbose=1,callbacks=[early_stopping])\n",
    "model.evaluate(test_x,test_y,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Results :\n",
    "\n",
    "The original results were the best, at 89.6% accuracy taking 22 epochs of about 11 seconds a piece.  When I increased the kernel and pool size as well as the size of the hidden layer, it stopped after only 15 epochs of about 13 seconds a piece and lost one percent accuracy.  Decreasing the kernel and pool size had the unfortunate effect of overfitting a little, since the training data measured up to 96% accuracy, but when tested on the test data, it only managed 89.65%.  While this is greater than the original accuracy, it not only isn't a very big improvement, but it required 59 epochs of about 45 seconds a piece, far longer than the original parameters.  This indicates to me that I shouldn't have used training accuracy as my indicator to stop training, but rather a validation of the test data.  The last set seems to have performed the second best, taking about twice as long as the original set and giving a better accuracy (one of 89.71%).  These took about 23 seconds a piece, and finished in just 24 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
