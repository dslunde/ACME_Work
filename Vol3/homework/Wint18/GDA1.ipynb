{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis\n",
    "    Darren Lund\n",
    "    Math 404\n",
    "    February 9, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy.linalg import inv\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Code up the Gaussian Discriminant Analysis algorithm.  Your code should have a .fit method that accepts a dataset X,y where y only takes on a finite number of values (classes), the .fit method should train the model (learn the parameters $\\pi_c$, $\\mu_c$, and  $\\Sigma_c$ for each class c, using the standard Gaussian MLE for each $\\mu_c$, and  $\\Sigma_c$ and using the estimate $\\pi_c$ = (#y=c)/N.  Your code should also have a `.predict_proba` method that accepts a data set X' and returns $p(y=c | x)$ for each x in X',  and it should have a `.predict` method that accepts data X' and returns the class prediction $\\hat{y}$ for each x in X' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GDA(object) :\n",
    "    def __init__(self) :\n",
    "        '''\n",
    "        Initialization of a Gaussian Discriminant Analysis classifier\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y) :\n",
    "        '''\n",
    "        A function for training the classifier.\n",
    "        \n",
    "        Input :\n",
    "            X - Training data\n",
    "            y - Classifications from data\n",
    "                (finite number of classes, counting representation)\n",
    "            \n",
    "        Output :\n",
    "            Instance of self with new data, namely pi, mu, and sigma for each class\n",
    "        '''\n",
    "        distinct_ys = set(y)\n",
    "        self.num_classes = len(distinct_ys)\n",
    "        self.pis = []\n",
    "        self.mus = []\n",
    "        self.sigs = []\n",
    "        self.sigs_inv = []\n",
    "        joined = np.hstack((X,np.array(y).reshape(-1,1)))\n",
    "        for i in range(self.num_classes) :\n",
    "            self.pis.append(list(y).count(i)/len(list(y)))\n",
    "            filt = X[joined[:,-1] == i]\n",
    "            self.mus.append(np.average(filt,axis=0))\n",
    "            self.sigs.append(np.cov(filt.T))\n",
    "            self.sigs_inv.append(inv(self.sigs[i]))\n",
    "    \n",
    "    def predict_proba(self,X) :\n",
    "        '''\n",
    "        Returns the probability of each class for the given data points in X\n",
    "        \n",
    "        Input :\n",
    "            X - Data to predict\n",
    "            \n",
    "        Output :\n",
    "            p(y=c|x) for each data point in X\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        probs = np.zeros((n,self.num_classes))\n",
    "        for j in range(n) :\n",
    "            prob = [self.pis[i]/sqrt(2*np.pi*np.linalg.det(self.sigs[i]))*np.exp(-0.5*(np.dot((X[j,:].T-self.mus[i]).T,np.dot(self.sigs_inv[i],X[j,:].T-self.mus[i])))) for i in range(self.num_classes)]\n",
    "            probs[j,:] = prob/sum(prob)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self,X) :\n",
    "        '''\n",
    "        Predicts the labels for a given dataset X\n",
    "        \n",
    "        Input :\n",
    "            X - Data to predict\n",
    "            \n",
    "        Output :\n",
    "            y_hat - The prediction of each data point in X\n",
    "        '''\n",
    "        probs = self.predict_proba(X)\n",
    "        y_hat = np.argmax(probs,axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Apply your GDA code to the cancer dataset with an appropriate train-test split and compare the results (train and test speed and test accuracy) to logistic regression and Naive Bayes.  Is one of these much better than the others?  Explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "tr_x, ts_x, tr_y, ts_y = train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gda = GDA()\n",
    "start = time()\n",
    "gda.fit(tr_x,tr_y)\n",
    "y_hat = gda.predict(ts_x)\n",
    "g_score = sum([y_hat[i] == ts_y[i] for i in range(len(y_hat))])/len(y_hat)\n",
    "end = time()\n",
    "g_time = end - start\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "start = time()\n",
    "lgr.fit(tr_x,tr_y)\n",
    "y_hat = lgr.predict(ts_x)\n",
    "l_score = sum([y_hat[i] == ts_y[i] for i in range(len(y_hat))])/len(y_hat)\n",
    "end = time()\n",
    "l_time = end - start\n",
    "\n",
    "nb = GaussianNB()\n",
    "start = time()\n",
    "nb.fit(tr_x,tr_y)\n",
    "y_hat = nb.predict(ts_x)\n",
    "n_score = sum([y_hat[i] == ts_y[i] for i in range(len(y_hat))])/len(y_hat)\n",
    "end = time()\n",
    "n_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTime\t\t\tScore\n",
      "GDA :\t0.19111394882202148\t0.941520467836\n",
      "LGR :\t0.13195395469665527\t0.93567251462\n",
      "NB  :\t0.004067182540893555\t0.923976608187\n"
     ]
    }
   ],
   "source": [
    "print('\\tTime\\t\\t\\tScore')\n",
    "print('GDA :\\t'+str(g_time)+'\\t'+str(g_score))\n",
    "print('LGR :\\t'+str(l_time)+'\\t'+str(l_score))\n",
    "print('NB  :\\t'+str(n_time)+'\\t'+str(n_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of speed, the Naive Bayes approach is by far the best.  However, it is also the least accurate, though not by much.  This is only true because of the size of the data, but for this data set, a small increase in time you can get almost an additional 2% accuracy increase by using the GDA.  Basically, I would say that no, there isn't one that's obviously better in all cases.  Assuming that you want quick results and are okay with a slgihtly lower accuracy, Naive Bayes is the way to go.  If accuracy is so important that you need that increase, however, and you have time on your hands, pick GDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Compare your train and test speed and your test accuracy to the `discriminant_analysis.QuadraticDiscriminantAnalysis` method in scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgda = QuadraticDiscriminantAnalysis()\n",
    "start = time()\n",
    "sgda.fit(tr_x,tr_y)\n",
    "y_hat = sgda.predict(ts_x)\n",
    "sg_score = sum([y_hat[i] == ts_y[i] for i in range(len(y_hat))])/len(y_hat)\n",
    "end = time()\n",
    "sg_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Time\t\t\tScore\n",
      "My GDA : 0.19111394882202148\t0.941520467836\n",
      "SK GDA : 1.3962910175323486\t0.941520467836\n"
     ]
    }
   ],
   "source": [
    "print('\\t Time\\t\\t\\tScore')\n",
    "print('My GDA : '+str(g_time)+'\\t'+str(g_score))\n",
    "print('SK GDA : '+str(sg_time)+'\\t'+str(sg_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
