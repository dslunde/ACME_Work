{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Assume that $X_1, X_2, ...$ is a sequence of iid Bernoulli random variables, and $X_i$ has probability $p$ of success.  Assume that $Y_1,Y_2, ...$ is another sequence of iid Bernoulli random variables, and every $Y_i$ has probability $q$ of success. Prove that if $p > q$, then $P\\left(\\frac{\\sum_{i=1}^n X_i}{n} > \\frac{1}{2}\\right) > P\\left(\\frac{\\sum_{i=1}^n Y_i}{n}  > \\frac{1}{2}\\right)$ for all $n$. Hint: write out a binomial expansion of the probabilities involved compare, term by term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that since $p > q$, we can write $p = q+\\delta$ for some $0 < \\delta \\leq 1-q$.  Let $Z_{i} \\sim Bern\\left(\\frac{\\delta}{1-q}\\right)$, and $W_{i} = min\\{1,Y_{i}+Z_{i}\\}$.  Then $W_{i}=0$ with probability $(1-q)\\left(1-\\frac{\\delta}{1-q}\\right)$ because $Y_{i}$ and $Z_{i}$ are independent.  However, $(1-q)\\left(1-\\frac{\\delta}{1-q}\\right) = (1-q)\\frac{1-q-\\delta}{1-q} = 1-q-\\delta = 1-(q+\\delta) = 1-p$ is the probability that $X_{i} = 0$, and thus $W_{i} \\sim X_{i}$.  Note that $X_{i} = W_{i} = 0$ implies $Y_{i} = 0$, but $Y_{i} = 0$ does not imply that $X_{i} = 0$ (because maybe $Z_{i} \\not= 0$).  Thus $\\{ i | Y_{i} > k\\} \\subset \\{ i | X_{i} > k\\}$ for all $k$.  Thus $P\\left(\\sum_{i=1}^{n} X_{i} > \\frac{n}{2}\\right) > P\\left(\\sum_{i=1}^{n} Y_{i} > \\frac{n}{2}\\right)$ for all $n$, which then gives $P\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_{i} > \\frac{1}{2}\\right) > P\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{i} > \\frac{1}{2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Assume that $X_i$ are iid Bernoulli random variables with probability $p â‰¥ \\frac{2}{3}$ of success.  Use Cramer's theorem to give a lower bound on the number $n$ needed to give 95% confidence that $\\sum_{i=1}^n \\frac{X_i}{n} > \\frac{1}{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The worst $n$ will come from having the smallest probability of success, so to find a lower bound on the $\\textit{needed}$ $n$, we can let $\\mu = \\frac{2}{3}$.  Cramer's Theorem states that $P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} X_{i} - \\mu\\right| \\geq \\epsilon \\right) < 2e^{-2\\epsilon^{2}n}$ for any $n$.  If we let $\\epsilon = \\frac{1}{6}$ and switch the inequality inside the probability to strictly less than, then that will guarantee that $\\frac{1}{n} \\sum_{i=1}^{n} X_{i} > \\frac{1}{2}$, so we have $P\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_{i} > \\frac{1}{2}\\right) > P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} X_{i} - \\frac{2}{3}\\right| \\leq \\frac{1}{6} \\right)= 1 - P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} X_{i} - \\frac{2}{3}\\right| \\geq \\frac{1}{6}\\right) > 1 - 2e^{\\frac{-2n}{36}} > 0.95 \\Rightarrow 0.05 > 2e^{\\frac{-2n}{36}} \\Rightarrow ln\\left(\\frac{1}{40}\\right) > -\\frac{n}{18} \\rightarrow n > 18ln(40) \\approx 66.39983$, so the minimal $n$ is $n = 67$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Use scikit learn's random forest classifier to predict survival for the titanic dataset (use an 80-20 train-test split). Experiment with `n_estimators` in range$(20,201,20)$ and `max_depth` in range$(2,10)$ and compare training time and prediction accuracy.  Also compare to the results you obtained last time with a singe tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset.\n",
    "data = pd.read_csv('titanic.csv')\n",
    "# Test train split.\n",
    "x = data.filter(['Pclass','Sex','Age'])\n",
    "x = x.fillna(0)\n",
    "x = x.replace(to_replace='male',value=1)\n",
    "x = x.replace(to_replace='female',value=0)\n",
    "y = list(data['Survived'])\n",
    "tr_x, ts_x, tr_y, ts_y = train_test_split(x,y,test_size=0.2)\n",
    "combo = []\n",
    "train_times = []\n",
    "accs = []\n",
    "# Experiment with max_depth and min_samples_leaf.\n",
    "for max_depth in range(2,10) :\n",
    "    for n_estimators in range(20,201,20) :\n",
    "        combo.append((max_depth,n_estimators))\n",
    "        clf = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators)\n",
    "        start = time()\n",
    "        clf.fit(tr_x,tr_y)\n",
    "        end = time()\n",
    "        train_times.append(end-start)\n",
    "        y_hat = clf.predict(ts_x)\n",
    "        accuracy = sum([int(ts_y[i] == y_hat[i]) for i in range(len(y_hat))])/len(y_hat)\n",
    "        accs.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth\tNum Estimators\tTime\t\t\tAccuracy\n",
      "2\t20\t\t0.10376787185668945\t0.7541899441340782\n",
      "2\t40\t\t0.06567215919494629\t0.7653631284916201\n",
      "2\t60\t\t0.08502602577209473\t0.7374301675977654\n",
      "2\t80\t\t0.12211394309997559\t0.770949720670391\n",
      "2\t100\t\t0.15097308158874512\t0.7653631284916201\n",
      "2\t120\t\t0.17398500442504883\t0.7653631284916201\n",
      "2\t140\t\t0.21190404891967773\t0.7430167597765364\n",
      "2\t160\t\t0.2398848533630371\t0.7653631284916201\n",
      "2\t180\t\t0.26813197135925293\t0.7653631284916201\n",
      "2\t200\t\t0.3300471305847168\t0.7653631284916201\n",
      "3\t20\t\t0.03299403190612793\t0.7877094972067039\n",
      "3\t40\t\t0.06352615356445312\t0.7821229050279329\n",
      "3\t60\t\t0.08589601516723633\t0.7877094972067039\n",
      "3\t80\t\t0.11332392692565918\t0.7821229050279329\n",
      "3\t100\t\t0.25925612449645996\t0.7877094972067039\n",
      "3\t120\t\t0.18869924545288086\t0.770949720670391\n",
      "3\t140\t\t0.23750972747802734\t0.8044692737430168\n",
      "3\t160\t\t0.39574289321899414\t0.7821229050279329\n",
      "3\t180\t\t0.4271259307861328\t0.8044692737430168\n",
      "3\t200\t\t0.32530713081359863\t0.7821229050279329\n",
      "4\t20\t\t0.033612966537475586\t0.7988826815642458\n",
      "4\t40\t\t0.06508898735046387\t0.776536312849162\n",
      "4\t60\t\t0.09371805191040039\t0.7988826815642458\n",
      "4\t80\t\t0.12829303741455078\t0.776536312849162\n",
      "4\t100\t\t0.15825605392456055\t0.7821229050279329\n",
      "4\t120\t\t0.1925818920135498\t0.776536312849162\n",
      "4\t140\t\t0.3245699405670166\t0.7877094972067039\n",
      "4\t160\t\t0.3001890182495117\t0.7877094972067039\n",
      "4\t180\t\t0.3403348922729492\t0.7821229050279329\n",
      "4\t200\t\t0.35927915573120117\t0.770949720670391\n",
      "5\t20\t\t0.0609889030456543\t0.7821229050279329\n",
      "5\t40\t\t0.08239603042602539\t0.7541899441340782\n",
      "5\t60\t\t0.09500813484191895\t0.7821229050279329\n",
      "5\t80\t\t0.12351298332214355\t0.7877094972067039\n",
      "5\t100\t\t0.15796685218811035\t0.7597765363128491\n",
      "5\t120\t\t0.1844339370727539\t0.770949720670391\n",
      "5\t140\t\t0.2242412567138672\t0.770949720670391\n",
      "5\t160\t\t0.25435400009155273\t0.7486033519553073\n",
      "5\t180\t\t0.28253912925720215\t0.770949720670391\n",
      "5\t200\t\t0.31407594680786133\t0.770949720670391\n",
      "6\t20\t\t0.034734249114990234\t0.7988826815642458\n",
      "6\t40\t\t0.06239008903503418\t0.776536312849162\n",
      "6\t60\t\t0.10094594955444336\t0.770949720670391\n",
      "6\t80\t\t0.12699317932128906\t0.7932960893854749\n",
      "6\t100\t\t0.16397881507873535\t0.770949720670391\n",
      "6\t120\t\t0.19790315628051758\t0.770949720670391\n",
      "6\t140\t\t0.22358298301696777\t0.7653631284916201\n",
      "6\t160\t\t0.30071496963500977\t0.7653631284916201\n",
      "6\t180\t\t0.29523777961730957\t0.7597765363128491\n",
      "6\t200\t\t0.31589698791503906\t0.770949720670391\n",
      "7\t20\t\t0.035623788833618164\t0.7877094972067039\n",
      "7\t40\t\t0.06446719169616699\t0.776536312849162\n",
      "7\t60\t\t0.0900881290435791\t0.770949720670391\n",
      "7\t80\t\t0.12757301330566406\t0.7821229050279329\n",
      "7\t100\t\t0.15256595611572266\t0.7821229050279329\n",
      "7\t120\t\t0.1876201629638672\t0.7821229050279329\n",
      "7\t140\t\t0.22834324836730957\t0.776536312849162\n",
      "7\t160\t\t0.2559041976928711\t0.7877094972067039\n",
      "7\t180\t\t0.30536317825317383\t0.776536312849162\n",
      "7\t200\t\t0.33142900466918945\t0.776536312849162\n",
      "8\t20\t\t0.04680204391479492\t0.7932960893854749\n",
      "8\t40\t\t0.06566095352172852\t0.7821229050279329\n",
      "8\t60\t\t0.09904670715332031\t0.7932960893854749\n",
      "8\t80\t\t0.1282660961151123\t0.7877094972067039\n",
      "8\t100\t\t0.1630568504333496\t0.7877094972067039\n",
      "8\t120\t\t0.196990966796875\t0.7877094972067039\n",
      "8\t140\t\t0.2224721908569336\t0.7821229050279329\n",
      "8\t160\t\t0.26382899284362793\t0.7932960893854749\n",
      "8\t180\t\t0.2881319522857666\t0.7877094972067039\n",
      "8\t200\t\t0.33426690101623535\t0.7821229050279329\n",
      "9\t20\t\t0.03257489204406738\t0.7597765363128491\n",
      "9\t40\t\t0.06831812858581543\t0.7653631284916201\n",
      "9\t60\t\t0.09739184379577637\t0.7877094972067039\n",
      "9\t80\t\t0.13046598434448242\t0.776536312849162\n",
      "9\t100\t\t0.16260910034179688\t0.8100558659217877\n",
      "9\t120\t\t0.3056063652038574\t0.7821229050279329\n",
      "9\t140\t\t0.25238895416259766\t0.7877094972067039\n",
      "9\t160\t\t0.3523671627044678\t0.7821229050279329\n",
      "9\t180\t\t0.2967209815979004\t0.7877094972067039\n",
      "9\t200\t\t0.34876418113708496\t0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "print('Depth\\tNum Estimators\\tTime\\t\\t\\tAccuracy')\n",
    "for i in range(len(combo)) :\n",
    "    print(str(combo[i][0])+'\\t'+str(combo[i][1])+'\\t\\t'+str(train_times[i])+'\\t'+str(accs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Do the same thing as #3, but on a large dataset related to your final project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../../../../Senior Project/DATA/'\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Walk through player files\n",
    "for dir_path , dir_name , file_names in os.walk(path) :\n",
    "    # 2017 will be our testing set\n",
    "    if '2017' in dir_path :\n",
    "        for name in file_names :\n",
    "            # Grab avgs file\n",
    "            if name[-4:] == 'avgs' :\n",
    "                data = pd.read_csv(os.path.join(dir_path,name))\n",
    "                if isinstance(test,list) :\n",
    "                    test = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "                else :\n",
    "                    test = np.vstack((test,data.drop(['Unnamed: 0'],axis=1)))\n",
    "    # Everything else will become our training set\n",
    "    else :\n",
    "        for name in file_names :\n",
    "            # Grab avgs file\n",
    "            if name[-4:] == 'avgs' :\n",
    "                data = pd.read_csv(os.path.join(dir_path,name))\n",
    "                if isinstance(train,list) :\n",
    "                    train = data.drop(['Unnamed: 0'],axis=1).as_matrix()\n",
    "                else :\n",
    "                    train = np.vstack((train,data.drop(['Unnamed: 0'],axis=1).as_matrix()))\n",
    "\n",
    "# From the way the data is saved, the last column is whether or not the player\n",
    "#     is a score on how much of a contributor he was during the season.\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo = []\n",
    "train_times = []\n",
    "accs = []\n",
    "# Experiment with max_depth and min_samples_leaf.\n",
    "for max_depth in range(2,10) :\n",
    "    for n_estimators in range(20,201,20) :\n",
    "        combo.append((max_depth,min_samples_leaf))\n",
    "        clf = RandomForestClassifier(max_depth=max_depth,n_estimators=n_estimators)\n",
    "        start = time()\n",
    "        clf.fit(train_x,train_y)\n",
    "        end = time()\n",
    "        train_times.append(end-start)\n",
    "        y_hat = clf.predict(test_x)\n",
    "        accuracy = sum([int(test_y[i] == y_hat[i]) for i in range(len(y_hat))])/len(y_hat)\n",
    "        accs.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth\tNum Estimators\tTime\t\t\tAccuracy\n",
      "2\t1\t\t0.09115767478942871\t0.8776041666666666\n",
      "2\t1\t\t0.12742924690246582\t0.8763020833333334\n",
      "2\t1\t\t0.19207096099853516\t0.8671875\n",
      "2\t1\t\t0.2530171871185303\t0.875\n",
      "2\t1\t\t0.3210580348968506\t0.875\n",
      "2\t1\t\t0.3770761489868164\t0.8736979166666666\n",
      "2\t1\t\t0.4574589729309082\t0.8736979166666666\n",
      "2\t1\t\t0.5234029293060303\t0.8723958333333334\n",
      "2\t1\t\t0.6811022758483887\t0.8736979166666666\n",
      "2\t1\t\t0.6395609378814697\t0.8736979166666666\n",
      "3\t1\t\t0.0816347599029541\t0.8671875\n",
      "3\t1\t\t0.15685606002807617\t0.8658854166666666\n",
      "3\t1\t\t0.23860406875610352\t0.87109375\n",
      "3\t1\t\t0.3051891326904297\t0.8723958333333334\n",
      "3\t1\t\t0.38082313537597656\t0.8723958333333334\n",
      "3\t1\t\t0.44976115226745605\t0.8723958333333334\n",
      "3\t1\t\t0.5460841655731201\t0.87109375\n",
      "3\t1\t\t0.6161069869995117\t0.8736979166666666\n",
      "3\t1\t\t0.6863839626312256\t0.8684895833333334\n",
      "3\t1\t\t0.776176929473877\t0.8736979166666666\n",
      "4\t1\t\t0.10170698165893555\t0.8763020833333334\n",
      "4\t1\t\t0.19461488723754883\t0.8723958333333334\n",
      "4\t1\t\t0.292248010635376\t0.87109375\n",
      "4\t1\t\t0.3707289695739746\t0.8658854166666666\n",
      "4\t1\t\t0.4629709720611572\t0.8684895833333334\n",
      "4\t1\t\t0.5535411834716797\t0.8723958333333334\n",
      "4\t1\t\t0.6356830596923828\t0.8697916666666666\n",
      "4\t1\t\t0.735947847366333\t0.8697916666666666\n",
      "4\t1\t\t0.8054909706115723\t0.8671875\n",
      "4\t1\t\t0.9138669967651367\t0.87109375\n",
      "5\t1\t\t0.1111140251159668\t0.8723958333333334\n",
      "5\t1\t\t0.21430706977844238\t0.8697916666666666\n",
      "5\t1\t\t0.3149838447570801\t0.8684895833333334\n",
      "5\t1\t\t0.41646695137023926\t0.8697916666666666\n",
      "5\t1\t\t0.5356740951538086\t0.8736979166666666\n",
      "5\t1\t\t0.6353740692138672\t0.87109375\n",
      "5\t1\t\t0.7621698379516602\t0.8658854166666666\n",
      "5\t1\t\t0.8583879470825195\t0.8671875\n",
      "5\t1\t\t0.9403767585754395\t0.87109375\n",
      "5\t1\t\t1.2820289134979248\t0.8671875\n",
      "6\t1\t\t0.15287184715270996\t0.8736979166666666\n",
      "6\t1\t\t0.25234508514404297\t0.8606770833333334\n",
      "6\t1\t\t0.44603490829467773\t0.8645833333333334\n",
      "6\t1\t\t0.581744909286499\t0.87890625\n",
      "6\t1\t\t0.6196990013122559\t0.87109375\n",
      "6\t1\t\t0.7851278781890869\t0.8645833333333334\n",
      "6\t1\t\t0.8259279727935791\t0.8658854166666666\n",
      "6\t1\t\t0.9603948593139648\t0.8697916666666666\n",
      "6\t1\t\t1.2447350025177002\t0.8697916666666666\n",
      "6\t1\t\t1.6658859252929688\t0.8736979166666666\n",
      "7\t1\t\t0.1611771583557129\t0.8736979166666666\n",
      "7\t1\t\t0.5139760971069336\t0.8802083333333334\n",
      "7\t1\t\t0.4075188636779785\t0.8684895833333334\n",
      "7\t1\t\t0.5373919010162354\t0.87109375\n",
      "7\t1\t\t0.7062020301818848\t0.8671875\n",
      "7\t1\t\t0.7807719707489014\t0.87109375\n",
      "7\t1\t\t0.9055569171905518\t0.87109375\n",
      "7\t1\t\t1.0273559093475342\t0.8671875\n",
      "7\t1\t\t1.1685409545898438\t0.8684895833333334\n",
      "7\t1\t\t1.3100192546844482\t0.87109375\n",
      "8\t1\t\t0.14293289184570312\t0.859375\n",
      "8\t1\t\t0.2807347774505615\t0.8658854166666666\n",
      "8\t1\t\t0.41791296005249023\t0.8684895833333334\n",
      "8\t1\t\t0.5467398166656494\t0.875\n",
      "8\t1\t\t0.7018589973449707\t0.875\n",
      "8\t1\t\t0.9052040576934814\t0.8671875\n",
      "8\t1\t\t1.1133692264556885\t0.875\n",
      "8\t1\t\t1.1574749946594238\t0.8697916666666666\n",
      "8\t1\t\t1.4007041454315186\t0.8736979166666666\n",
      "8\t1\t\t1.7292311191558838\t0.8736979166666666\n",
      "9\t1\t\t0.21022796630859375\t0.8619791666666666\n",
      "9\t1\t\t0.3526480197906494\t0.8671875\n",
      "9\t1\t\t0.4367377758026123\t0.8736979166666666\n",
      "9\t1\t\t0.5858197212219238\t0.8658854166666666\n",
      "9\t1\t\t0.7602229118347168\t0.86328125\n",
      "9\t1\t\t0.9287619590759277\t0.8671875\n",
      "9\t1\t\t1.0969901084899902\t0.875\n",
      "9\t1\t\t1.2399790287017822\t0.87109375\n",
      "9\t1\t\t1.3467152118682861\t0.875\n",
      "9\t1\t\t1.4649720191955566\t0.8684895833333334\n"
     ]
    }
   ],
   "source": [
    "print('Depth\\tNum Estimators\\tTime\\t\\t\\tAccuracy')\n",
    "for i in range(len(combo)) :\n",
    "    print(str(combo[i][0])+'\\t'+str(combo[i][1])+'\\t\\t'+str(train_times[i])+'\\t'+str(accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
