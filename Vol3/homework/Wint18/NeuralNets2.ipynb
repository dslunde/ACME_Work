{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist as fmn\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Experiment with fully connected neural nets for classification of the Fashion-MNIST data: add at least two more layers, make all hidden layers at least 20 neurons wide, and try it with both ReLU and sigmoid activations.  Train for as many epochs as you need until the loss function (categorical cross entropy) stops improving--Keras's `callbacks.EarlyStopping` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_keras_model(optimizer,activ) :\n",
    "    output_dim = 10\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=input_dim, activation=activ))\n",
    "    model.add(Dense(40, input_dim=input_dim, activation=activ))\n",
    "    model.add(Dense(20, input_dim=input_dim, activation=activ))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Generate optimizers for each run\n",
    "def generate_optimizers(learning_rate=.01):\n",
    "    \"\"\"Generate a dictionary of optimizers for keras to use in gridsearch\"\"\"\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    optimizer_dict = {'Adam':adam}\n",
    "    return optimizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 5us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 21s 1us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 7s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fmn.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "rates = [0.0001,0.001,.01,.1,1]\n",
    "meth_list = ['Adam']\n",
    "\n",
    "# Reshape y as one-hot\n",
    "y_train = to_categorical(y_train, 10).astype(float)\n",
    "y_test = to_categorical(y_test, 10).astype(float)\n",
    "\n",
    "# Flatten images\n",
    "input_dim = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], input_dim))\n",
    "x_test = x_test.reshape((x_test.shape[0], input_dim))\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "# Permute the training data to be in random order\n",
    "perm = np.random.permutation(x_train.shape[0])\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 1.3460 - acc: 0.5547 - val_loss: 0.8014 - val_acc: 0.7407\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.6437 - acc: 0.7818 - val_loss: 0.5873 - val_acc: 0.7951\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.5206 - acc: 0.8185 - val_loss: 0.5242 - val_acc: 0.8136\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.4713 - acc: 0.8333 - val_loss: 0.4916 - val_acc: 0.8251\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.4425 - acc: 0.8422 - val_loss: 0.4708 - val_acc: 0.8318\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.4228 - acc: 0.8488 - val_loss: 0.4561 - val_acc: 0.8357\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.4080 - acc: 0.8543 - val_loss: 0.4446 - val_acc: 0.8399\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.3961 - acc: 0.8582 - val_loss: 0.4353 - val_acc: 0.8438\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.3862 - acc: 0.8616 - val_loss: 0.4278 - val_acc: 0.8472\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.3777 - acc: 0.8641 - val_loss: 0.4213 - val_acc: 0.8495\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.3704 - acc: 0.8669 - val_loss: 0.4158 - val_acc: 0.8518\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.3641 - acc: 0.8689 - val_loss: 0.4112 - val_acc: 0.8530\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.3584 - acc: 0.8708 - val_loss: 0.4073 - val_acc: 0.8549\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.3533 - acc: 0.8726 - val_loss: 0.4039 - val_acc: 0.8560\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.3486 - acc: 0.8743 - val_loss: 0.4009 - val_acc: 0.8569\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.3443 - acc: 0.8758 - val_loss: 0.3983 - val_acc: 0.8583\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.3404 - acc: 0.8774 - val_loss: 0.3961 - val_acc: 0.8590\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.3367 - acc: 0.8785 - val_loss: 0.3940 - val_acc: 0.8600\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.3332 - acc: 0.8799 - val_loss: 0.3921 - val_acc: 0.8610\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.3300 - acc: 0.8809 - val_loss: 0.3905 - val_acc: 0.8615\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 2.1441 - acc: 0.3566 - val_loss: 1.9930 - val_acc: 0.4498\n",
      "Epoch 2/20\n",
      " - 1s - loss: 1.8566 - acc: 0.4827 - val_loss: 1.7381 - val_acc: 0.5219\n",
      "Epoch 3/20\n",
      " - 1s - loss: 1.6302 - acc: 0.5751 - val_loss: 1.5411 - val_acc: 0.5990\n",
      "Epoch 4/20\n",
      " - 1s - loss: 1.4515 - acc: 0.6249 - val_loss: 1.3814 - val_acc: 0.6612\n",
      "Epoch 5/20\n",
      " - 1s - loss: 1.3032 - acc: 0.7022 - val_loss: 1.2468 - val_acc: 0.7211\n",
      "Epoch 6/20\n",
      " - 1s - loss: 1.1767 - acc: 0.7431 - val_loss: 1.1320 - val_acc: 0.7463\n",
      "Epoch 7/20\n",
      " - 1s - loss: 1.0685 - acc: 0.7661 - val_loss: 1.0341 - val_acc: 0.7660\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.9752 - acc: 0.7816 - val_loss: 0.9497 - val_acc: 0.7793\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.8942 - acc: 0.7914 - val_loss: 0.8767 - val_acc: 0.7852\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.8242 - acc: 0.7978 - val_loss: 0.8144 - val_acc: 0.7880\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.7647 - acc: 0.8034 - val_loss: 0.7623 - val_acc: 0.7925\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.7148 - acc: 0.8089 - val_loss: 0.7190 - val_acc: 0.7977\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.6730 - acc: 0.8139 - val_loss: 0.6828 - val_acc: 0.8038\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.6377 - acc: 0.8196 - val_loss: 0.6522 - val_acc: 0.8082\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.6076 - acc: 0.8254 - val_loss: 0.6261 - val_acc: 0.8123\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.5815 - acc: 0.8302 - val_loss: 0.6034 - val_acc: 0.8162\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.5586 - acc: 0.8346 - val_loss: 0.5837 - val_acc: 0.8207\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.5386 - acc: 0.8387 - val_loss: 0.5666 - val_acc: 0.8243\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.5209 - acc: 0.8422 - val_loss: 0.5516 - val_acc: 0.8265\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.5054 - acc: 0.8451 - val_loss: 0.5386 - val_acc: 0.8299\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.6200 - acc: 0.7864 - val_loss: 0.4480 - val_acc: 0.8376\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.3903 - acc: 0.8585 - val_loss: 0.4108 - val_acc: 0.8534\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.3561 - acc: 0.8709 - val_loss: 0.3924 - val_acc: 0.8593\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.3354 - acc: 0.8782 - val_loss: 0.3828 - val_acc: 0.8624\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.3195 - acc: 0.8837 - val_loss: 0.3777 - val_acc: 0.8660\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3066 - acc: 0.8881 - val_loss: 0.3740 - val_acc: 0.8690\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.2963 - acc: 0.8916 - val_loss: 0.3704 - val_acc: 0.8707\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.2879 - acc: 0.8951 - val_loss: 0.3703 - val_acc: 0.8698\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.2802 - acc: 0.8974 - val_loss: 0.3714 - val_acc: 0.8690\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.2737 - acc: 0.9003 - val_loss: 0.3731 - val_acc: 0.8710\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.9922 - acc: 0.6931 - val_loss: 0.6338 - val_acc: 0.7887\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.5460 - acc: 0.8120 - val_loss: 0.5352 - val_acc: 0.8157\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.4729 - acc: 0.8416 - val_loss: 0.4958 - val_acc: 0.8312\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.4313 - acc: 0.8563 - val_loss: 0.4747 - val_acc: 0.8384\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.4043 - acc: 0.8647 - val_loss: 0.4605 - val_acc: 0.8426\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3849 - acc: 0.8707 - val_loss: 0.4522 - val_acc: 0.8447\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.3695 - acc: 0.8751 - val_loss: 0.4470 - val_acc: 0.8470\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.3569 - acc: 0.8793 - val_loss: 0.4432 - val_acc: 0.8486\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.3461 - acc: 0.8831 - val_loss: 0.4401 - val_acc: 0.8490\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.3368 - acc: 0.8865 - val_loss: 0.4371 - val_acc: 0.8504\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.3287 - acc: 0.8891 - val_loss: 0.4350 - val_acc: 0.8508\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.3217 - acc: 0.8913 - val_loss: 0.4319 - val_acc: 0.8523\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.3155 - acc: 0.8933 - val_loss: 0.4304 - val_acc: 0.8534\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.3098 - acc: 0.8951 - val_loss: 0.4289 - val_acc: 0.8542\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.3047 - acc: 0.8964 - val_loss: 0.4285 - val_acc: 0.8549\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.2998 - acc: 0.8979 - val_loss: 0.4277 - val_acc: 0.8554\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.2953 - acc: 0.8997 - val_loss: 0.4266 - val_acc: 0.8550\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.2912 - acc: 0.9013 - val_loss: 0.4272 - val_acc: 0.8539\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.2872 - acc: 0.9028 - val_loss: 0.4279 - val_acc: 0.8545\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5079 - acc: 0.8154 - val_loss: 0.4569 - val_acc: 0.8354\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.3949 - acc: 0.8574 - val_loss: 0.4465 - val_acc: 0.8424\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.3726 - acc: 0.8660 - val_loss: 0.4429 - val_acc: 0.8437\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.3579 - acc: 0.8715 - val_loss: 0.4386 - val_acc: 0.8496\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.3477 - acc: 0.8765 - val_loss: 0.4427 - val_acc: 0.8516\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3368 - acc: 0.8807 - val_loss: 0.4311 - val_acc: 0.8561\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.3348 - acc: 0.8817 - val_loss: 0.4332 - val_acc: 0.8512\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.3256 - acc: 0.8855 - val_loss: 0.4465 - val_acc: 0.8491\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.7529 - acc: 0.7110 - val_loss: 0.6146 - val_acc: 0.7764\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.5458 - acc: 0.8031 - val_loss: 0.5439 - val_acc: 0.8061\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.4958 - acc: 0.8207 - val_loss: 0.5189 - val_acc: 0.8100\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.4700 - acc: 0.8290 - val_loss: 0.4944 - val_acc: 0.8220\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.4543 - acc: 0.8334 - val_loss: 0.4831 - val_acc: 0.8253\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.4385 - acc: 0.8384 - val_loss: 0.4804 - val_acc: 0.8266\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.4266 - acc: 0.8428 - val_loss: 0.4683 - val_acc: 0.8296\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.4208 - acc: 0.8447 - val_loss: 0.4684 - val_acc: 0.8326\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.4157 - acc: 0.8469 - val_loss: 0.4631 - val_acc: 0.8341\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.4016 - acc: 0.8546 - val_loss: 0.4605 - val_acc: 0.8351\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.3964 - acc: 0.8560 - val_loss: 0.4520 - val_acc: 0.8357\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.3850 - acc: 0.8593 - val_loss: 0.4548 - val_acc: 0.8343\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.3931 - acc: 0.8551 - val_loss: 0.4483 - val_acc: 0.8400\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.3831 - acc: 0.8597 - val_loss: 0.4495 - val_acc: 0.8401\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.3725 - acc: 0.8648 - val_loss: 0.4349 - val_acc: 0.8490\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.3729 - acc: 0.8635 - val_loss: 0.4373 - val_acc: 0.8471\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.3696 - acc: 0.8648 - val_loss: 0.4415 - val_acc: 0.8436\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 1.8959 - acc: 0.2606 - val_loss: 1.6547 - val_acc: 0.3022\n",
      "Epoch 2/20\n",
      " - 1s - loss: 1.6794 - acc: 0.2932 - val_loss: 1.6171 - val_acc: 0.3295\n",
      "Epoch 3/20\n",
      " - 1s - loss: 1.5628 - acc: 0.3445 - val_loss: 1.4845 - val_acc: 0.3841\n",
      "Epoch 4/20\n",
      " - 1s - loss: 2.2460 - acc: 0.1452 - val_loss: 2.3083 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 1s - loss: 2.3085 - acc: 0.1002 - val_loss: 2.3080 - val_acc: 0.1000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 2.4280 - acc: 0.1001 - val_loss: 2.3380 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 1s - loss: 2.3452 - acc: 0.1004 - val_loss: 2.3312 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 1s - loss: 2.3415 - acc: 0.1012 - val_loss: 2.3319 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 1s - loss: 2.3400 - acc: 0.1013 - val_loss: 2.3323 - val_acc: 0.1000\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 15.0233 - acc: 0.0664 - val_loss: 15.0898 - val_acc: 0.0638\n",
      "Epoch 2/20\n",
      " - 1s - loss: 15.0436 - acc: 0.0667 - val_loss: 15.0898 - val_acc: 0.0638\n",
      "Epoch 3/20\n",
      " - 1s - loss: 15.0436 - acc: 0.0667 - val_loss: 15.0898 - val_acc: 0.0638\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 14.4638 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 1s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 1s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "relu_times = []\n",
    "sigm_times = []\n",
    "\n",
    "relu_acc = []\n",
    "sigm_acc = []\n",
    "\n",
    "for rate in rates :\n",
    "    methods = generate_optimizers(rate)\n",
    "    for meth in methods.keys() :\n",
    "        method = methods[meth]\n",
    "        model1 = build_keras_model(method,'relu')\n",
    "        model2 = build_keras_model(method,'sigmoid')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "        start = time()\n",
    "        hist1 = model1.fit(x_train,\n",
    "                y_train,\n",
    "                batch_size=128,\n",
    "                epochs=20,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=[early_stopping])\n",
    "        end = time()\n",
    "        relu_times.append(end-start)\n",
    "        start = time()\n",
    "        hist2 = model2.fit(x_train,\n",
    "                y_train,\n",
    "                batch_size=128,\n",
    "                epochs=20,\n",
    "                verbose=2,\n",
    "                shuffle=False,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=[early_stopping])\n",
    "        end = time()\n",
    "        sigm_times.append(end-start)\n",
    "        relu_acc.append(max(hist1.history['acc']))\n",
    "        sigm_acc.append(max(hist2.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRate\tMethod\tTime\t\t\tAccuracy\n",
      "relu:\t0.0001\tAdam\t17.273399591445923\t0.8808833333333334\n",
      "sigm:\t0.0001\tAdam\t17.2337327003479\t0.8450666666984558\n",
      "relu:\t0.001\tAdam\t8.9581618309021\t\t0.90035\n",
      "sigm:\t0.001\tAdam\t16.682024717330933\t0.9027999999682108\n",
      "relu:\t0.01\tAdam\t7.4131529331207275\t0.8854833333015442\n",
      "sigm:\t0.01\tAdam\t15.32559609413147\t0.8648499999682109\n",
      "relu:\t0.1\tAdam\t4.91444993019104\t0.3445333333492279\n",
      "sigm:\t0.1\tAdam\t4.097715377807617\t0.10125000000794729\n",
      "relu:\t1\tAdam\t3.298017740249634\t0.06666666666269302\n",
      "sigm:\t1\tAdam\t3.419442653656006\t0.10000000000794729\n"
     ]
    }
   ],
   "source": [
    "print('\\tRate\\tMethod\\tTime\\t\\t\\tAccuracy')\n",
    "n = len(relu_acc)\n",
    "for i in range(n) :\n",
    "    if i == 1 :\n",
    "        print('relu:\\t'+str(rates[i//(n//5)])+'\\t'+str(meth_list[0])+'\\t'+str(relu_times[i])+'\\t\\t'+str(relu_acc[i]))\n",
    "    else :\n",
    "        print('relu:\\t'+str(rates[i//(n//5)])+'\\t'+str(meth_list[0])+'\\t'+str(relu_times[i])+'\\t'+str(relu_acc[i]))\n",
    "    print('sigm:\\t'+str(rates[i//(n//5)])+'\\t'+str(meth_list[0])+'\\t'+str(sigm_times[i])+'\\t'+str(sigm_acc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Using the notation from class today (and from the video) calculate one iteration of backpropagation by hand (or code something to do it for you).  That is, calculate the forward pass and then the backward pass to compute both the output of the network for the current weights, and the gradient (with respect to the Ws and the bs) on a (fully connected) neural network with two hidden layers of 2 neurons each (ReLu activation), two inputs, and a single output layer having a sigmoid activation function. Use the input data $x=(1,-1)$, $y=1$, and assume the current weights are\n",
    "\n",
    "$W^1 = \\left[ \\begin{matrix} 0.25 & 0.1 \\\\ -0.2  & 0.9 \\end{matrix} \\right]$\n",
    "\n",
    "$b^1 =   \\left[ \\begin{matrix} 0.1 \\\\ -0.2 \\end{matrix} \\right]$\n",
    "\n",
    "$W^2 =  \\left[ \\begin{matrix} 0.5 & 0.8 \\\\ 0.3 & 0.7 \\end{matrix} \\right]$\n",
    "\n",
    "$b^2 =  \\left[ \\begin{matrix} -0.3 \\\\ 0.1 \\end{matrix} \\right]$\n",
    "\n",
    "$W^3 =  \\left[ \\begin{matrix} 0.1 & -0.2 \\end{matrix} \\right]$\n",
    "\n",
    "$b^3 = 0.3$\n",
    "\n",
    "So the structure of the network looks something like this:\n",
    "\n",
    "            L_1      L_2      L_3\n",
    "            \n",
    "    x_0 ----> O ----> O ----> O ----> \n",
    "\n",
    "        \\  /        \\  /           /\n",
    "       \n",
    "         / \\         / \\         /\n",
    "        \n",
    "          x_1 ----> O ----> O\n",
    "\n",
    "where the Os here represent neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$L_{1}$ :\n",
    "$\n",
    "\\begin{align*}\n",
    "z^1 &= w^1a^0+b^1 \\\\\n",
    "&= \\left[ \\begin{matrix} 0.25 & 0.1 \\\\ -0.2  & 0.9 \\end{matrix} \\right] \\left[ \\begin{matrix} 1 \\\\ -1 \\end{matrix} \\right] + \\left[ \\begin{matrix} 0.1 \\\\ -0.2 \\end{matrix} \\right] \\\\\n",
    "&= \\left[ \\begin{matrix} 0.25 \\\\ -1.3 \\end{matrix} \\right] \\\\\n",
    "\\hat{\\sigma}(z^1) &= \\left[ \\begin{matrix} 0.25 \\\\ 0 \\end{matrix} \\right] = a^1 \\\\\n",
    "\\hat{\\sigma}'(z^1) &= \\left[ \\begin{matrix} 1 \\\\ 0 \\end{matrix} \\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{2}$ :\n",
    "$\n",
    "\\begin{align*}\n",
    "z^2 &= w^2a^1+b^2 \\\\\n",
    "&= \\left[ \\begin{matrix} 0.5 & 0.8 \\\\ 0.3 & 0.7 \\end{matrix} \\right] \\left[ \\begin{matrix} 0.25 \\\\ 0 \\end{matrix} \\right] + \\left[ \\begin{matrix} -0.3 \\\\ 0.1 \\end{matrix} \\right] \\\\\n",
    "&= \\left[ \\begin{matrix} -0.175 \\\\ 0.175 \\end{matrix} \\right] \\\\\n",
    "\\hat{\\sigma}(z^2) &= \\left[ \\begin{matrix} 0 \\\\ 0.175 \\end{matrix} \\right] = a^2 \\\\\n",
    "\\hat{\\sigma}'(z^2) &= \\left[ \\begin{matrix} 0 \\\\ 1 \\end{matrix} \\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{3}$ :\n",
    "$\n",
    "\\begin{align*}\n",
    "z^3 &= w^3a^2+b^3 \\\\\n",
    "&= \\left[ \\begin{matrix} 0.1 & -0.2 \\end{matrix} \\right] \\left[ \\begin{matrix} 0.25 \\\\ 0 \\end{matrix} \\right] + 0.3 \\\\\n",
    "&= 0.265 \\\\\n",
    "\\hat{\\sigma}(z^3) &= \\frac{1}{1+e^{-0.265}} \\approx 0.565865 = \\hat{y} \\\\\n",
    "\\hat{\\sigma}'(z^3) &= \\frac{e^{-0.265}}{\\left(1+e^{-0.265}\\right)^{2}} \\approx 0.24566\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C = \\frac{1}{2} \\left|\\left| \\hat{y} - y \\right|\\right|_{2}^{2} = \\frac{1}{2} \\left|\\left|1-\\frac{1}{1+e^{-0.265}}\\right|\\right|_{2}^{2} = \\frac{1}{2}\\left|\\frac{e^{-0.265}}{1+e^{-0.265}}\\right|^{2} \\approx \\frac{1}{2}\\left(0.434135\\right)^{2} \\approx 0.094236597855$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W_{1}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)W^3\\hat{\\sigma}'(z^2)W^2\\hat{\\sigma}'(z^1)(a^0)^{T} \\\\\n",
    "&= (0.434135)(0.24566)\\left[\\begin{matrix}0.1&-0.2\\end{matrix}\\right]\\left[\\begin{matrix}0\\\\1\\end{matrix}\\right]\\left[\\begin{matrix}0.5&0.8\\\\0.3&0.7\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\0\\end{matrix}\\right]\\left[\\begin{matrix}1&-1\\end{matrix}\\right] \\\\\n",
    "&\\approx \\left[ \\begin{matrix} -0.01066504 & 0.01066504 \\\\ -0.006399025 & 0.006399025 \\end{matrix} \\right] \\\\\n",
    "\\frac{\\partial C}{\\partial b_{1}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)W^3\\hat{\\sigma}'(z^2)W^2\\hat{\\sigma}'(z^1)1 \\\\\n",
    "&= (0.434135)(0.24566)\\left[\\begin{matrix}0.1&-0.2\\end{matrix}\\right]\\left[\\begin{matrix}0\\\\1\\end{matrix}\\right]\\left[\\begin{matrix}0.5&0.8\\\\0.3&0.7\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\0\\end{matrix}\\right] \\\\\n",
    "&\\approx \\left[ \\begin{matrix} -0.01066504 \\\\ -0.006399025 \\end{matrix} \\right]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W_{2}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)W^3\\hat{\\sigma}'(z^2)(a^1)^{T} \\\\\n",
    "&= (0.434135)(0.24566)\\left[\\begin{matrix}0.1&-0.2\\end{matrix}\\right]\\left[\\begin{matrix}0\\\\1\\end{matrix}\\right]\\left[\\begin{matrix}0.25&0\\end{matrix}\\right] \\\\\n",
    "&\\approx \\left[ \\begin{matrix} -0.00533252 & 0 \\end{matrix} \\right] \\\\\n",
    "\\frac{\\partial C}{\\partial b_{2}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)W^3\\hat{\\sigma}'(z^2)1 \\\\\n",
    "&= (0.434135)(0.24566)\\left[\\begin{matrix}0.1&-0.2\\end{matrix}\\right]\\left[\\begin{matrix}0\\\\1\\end{matrix}\\right] \\\\\n",
    "&\\approx -0.0213308\n",
    "\\end{align*}\n",
    "$  \n",
    "However, these have the wrong dimension, so I don't know what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W_{3}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)(a^2)^{T} \\\\\n",
    "&= (0.434135)(0.24566)\\left[\\begin{matrix}0.1&-0.2\\end{matrix}\\right] \\\\\n",
    "&\\approx \\left[ \\begin{matrix} 0.0106650385 & 0.02133007708 \\end{matrix} \\right] \\\\\n",
    "\\frac{\\partial C}{\\partial b_{3}} &= (\\hat{y}-y)\\hat{\\sigma}'(z^3)1 \\\\\n",
    "&\\approx 0.106650385\n",
    "\\end{align*}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
